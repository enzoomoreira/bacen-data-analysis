{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ea54b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CÉLULA 1: Configuração do Ambiente e Funções Auxiliares\n",
    "# ==============================================================================\n",
    "# Descrição: Importa bibliotecas, define constantes globais (datas, caminhos),\n",
    "# e cria funções de utilidade que serão usadas ao longo do notebook.\n",
    "# ==============================================================================\n",
    "import pandas as pd\n",
    "import requests\n",
    "import zipfile\n",
    "import warnings\n",
    "import re\n",
    "from pathlib import Path\n",
    "from bacen_analysis import standardize_cnpj_base8\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d33ba97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configurações de Datas e Diretórios ---\n",
    "# Use datas fixas para reprodutibilidade ou ajuste para datas dinâmicas.\n",
    "# Ex: end=pd.Timestamp.now().strftime('%Y-%m-%d')\n",
    "DATE_START = '2025-01-01'\n",
    "DATE_END   = '2025-06-30'\n",
    "\n",
    "date_range_monthly = pd.date_range(start=DATE_START, end=DATE_END, freq='M').strftime(\"%Y%m\").tolist()\n",
    "date_range_quarterly = pd.date_range(start=DATE_START, end=DATE_END, freq='Q').strftime(\"%Y%m\").tolist()\n",
    "\n",
    "# Estrutura de diretórios\n",
    "base_dir    = Path('../..').resolve() # O notebook está em notebooks/etl/, então sobe dois níveis para a raiz\n",
    "dir_data    = base_dir / 'data'\n",
    "dir_inputs  = dir_data / 'input'\n",
    "dir_outputs = dir_data / 'output'\n",
    "for d in (dir_data, dir_inputs, dir_outputs):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Diretório Base: {base_dir}\")\n",
    "print(f\"Diretório de Dados: {dir_data}\")\n",
    "print(f\"Diretório de Inputs: {dir_inputs}\")\n",
    "print(f\"Diretório de Outputs: {dir_outputs}\")\n",
    "print(f\"\\nPeríodo de Análise Mensal: de {date_range_monthly[0]} a {date_range_monthly[-1]}\")\n",
    "print(f\"Período de Análise Trimestral: de {date_range_quarterly[0]} a {date_range_quarterly[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0ee1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Funções Auxiliares Locais ---\n",
    "\n",
    "def clean_text_column(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Limpa uma coluna de texto, removendo caracteres de controle e espaços extras.\"\"\"\n",
    "    if series.empty:\n",
    "        return series\n",
    "    \n",
    "    def remove_illegal_chars(text: str) -> str:\n",
    "        if pd.isna(text): return ''\n",
    "        # Remove caracteres de controle ASCII (exceto tab, newline, etc. que já são tratados pelo strip/replace)\n",
    "        return re.sub(r'[\\x00-\\x1F\\x7F]', '', str(text))\n",
    "\n",
    "    return series.astype(str).apply(remove_illegal_chars).str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "\n",
    "def create_data_infos(df: pd.DataFrame, df_name: str) -> pd.DataFrame:\n",
    "    \"\"\"Cria um perfil de dados (info) a partir de um DataFrame.\"\"\"\n",
    "    print(f\"Gerando perfil do DataFrame: '{df_name}'...\")\n",
    "    \n",
    "    dict_df = pd.DataFrame({\n",
    "        'Coluna': df.columns,\n",
    "        'Tipo de Dado (Dtype)': df.dtypes.astype(str),\n",
    "        'Valores Não Nulos': df.count().values,\n",
    "        'Valores Nulos': df.isnull().sum().values,\n",
    "    })\n",
    "    dict_df['% Nulos'] = (dict_df['Valores Nulos'] / len(df) * 100).round(2)\n",
    "    \n",
    "    def get_example(col):\n",
    "        try:\n",
    "            return df[col].dropna().unique()[:3]\n",
    "        except Exception:\n",
    "            return \"Não foi possível extrair exemplos\"\n",
    "            \n",
    "    dict_df['Exemplos'] = [get_example(col) for col in df.columns]\n",
    "    \n",
    "    return dict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7561bd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pipeline para download e processamento de dados COSIF ---\n",
    "\n",
    "def pipeline_cosif(\n",
    "    tipo_fonte: str,\n",
    "    url_path_segment: str,\n",
    "    file_name_pattern: str,\n",
    "    sufixos: list,\n",
    "    datas: list,\n",
    "    dir_input_base: Path,\n",
    "    dir_output_base: Path\n",
    "):\n",
    "    \"\"\"\n",
    "    Executa um pipeline de ETL genérico para uma fonte de dados COSIF.\n",
    "\n",
    "    Esta função baixa, extrai, lê, limpa e salva os dados, encapsulando a\n",
    "    lógica que antes era repetida para as fontes Individual e Prudencial.\n",
    "\n",
    "    Args:\n",
    "        tipo_fonte (str): Nome descritivo da fonte (ex: 'Individual' ou 'Prudencial').\n",
    "        url_path_segment (str): O segmento do caminho na URL do BCB (ex: 'Bancos').\n",
    "        file_name_pattern (str): O padrão do nome do arquivo (ex: 'BANCOS' ou 'BLOPRUDENCIAL').\n",
    "        sufixos (list): Lista de sufixos de arquivo a serem tentados.\n",
    "        datas (list): Lista de datas no formato YYYYMM para baixar.\n",
    "        dir_input_base (Path): O diretório 'Input' principal.\n",
    "        dir_output_base (Path): O diretório 'Output' principal.\n",
    "    \"\"\"\n",
    "    print(f\"\\n>>> INICIANDO PIPELINE: COSIF {tipo_fonte.upper()} <<<\")\n",
    "\n",
    "    # --- 1. Download e Extração ---\n",
    "    cosif_dir = dir_input_base / 'COSIF' / tipo_fonte.lower()\n",
    "    cosif_dir.mkdir(parents=True, exist_ok=True)\n",
    "    all_csv_paths = []\n",
    "\n",
    "    for date in datas:\n",
    "        subfolder = cosif_dir / date\n",
    "        subfolder.mkdir(exist_ok=True)\n",
    "        \n",
    "        existing_csvs = list(subfolder.glob(f\"*{file_name_pattern}*.csv\"))\n",
    "        if existing_csvs:\n",
    "            print(f\"COSIF {tipo_fonte} ({date}): CSV já existe, pulando download.\")\n",
    "            all_csv_paths.extend(existing_csvs)\n",
    "            continue\n",
    "\n",
    "        download_success = False\n",
    "        error_messages = []\n",
    "        \n",
    "        print(f\"COSIF {tipo_fonte} ({date}): Buscando arquivo...\", end='')\n",
    "\n",
    "        for suffix in sufixos:\n",
    "            if download_success: break\n",
    "            \n",
    "            url = f\"https://www.bcb.gov.br/content/estabilidadefinanceira/cosif/{url_path_segment}/{date}{suffix}\"\n",
    "            print(f\"\\rCOSIF {tipo_fonte} ({date}): Tentando sufixo '{suffix}'...\", end='')\n",
    "\n",
    "            try:\n",
    "                resp = requests.get(url, timeout=90)\n",
    "                resp.raise_for_status()\n",
    "                content_type = resp.headers.get('Content-Type', '').lower()\n",
    "                if 'html' in content_type:\n",
    "                    error_messages.append(f\"'{suffix}': Recebido conteúdo HTML.\")\n",
    "                    continue\n",
    "\n",
    "                local_file = subfolder / f\"{date}{suffix}\"\n",
    "                with open(local_file, 'wb') as f: f.write(resp.content)\n",
    "\n",
    "                if 'zip' in suffix.lower():\n",
    "                    with zipfile.ZipFile(local_file, 'r') as zf:\n",
    "                        extracted_files = [\n",
    "                            member for member in zf.namelist()\n",
    "                            if member.lower().endswith('.csv') and file_name_pattern.lower() in member.lower()\n",
    "                        ]\n",
    "                        if extracted_files:\n",
    "                            zf.extractall(subfolder, members=extracted_files)\n",
    "                            all_csv_paths.extend([subfolder / fname for fname in extracted_files])\n",
    "                            download_success = True\n",
    "                        else:\n",
    "                            error_messages.append(f\"'{suffix}': ZIP não continha o CSV esperado.\")\n",
    "                elif '.csv' in suffix.lower():\n",
    "                    all_csv_paths.append(local_file)\n",
    "                    download_success = True\n",
    "\n",
    "            except requests.exceptions.HTTPError as e: error_messages.append(f\"'{suffix}': Não encontrado (Status {e.response.status_code})\")\n",
    "            except Exception as e: error_messages.append(f\"'{suffix}': Erro inesperado ({e})\")\n",
    "\n",
    "        print(\"\\r\" + \" \" * 80 + \"\\r\", end=\"\")\n",
    "        if download_success:\n",
    "            print(f\"COSIF {tipo_fonte} ({date}): Sucesso! Arquivo baixado e extraído.\")\n",
    "        else:\n",
    "            print(f\"COSIF {tipo_fonte} ({date}): FALHA no download. Motivos: {'; '.join(error_messages)}\")\n",
    "\n",
    "    # --- 2. Leitura e Consolidação ---\n",
    "    df_list = []\n",
    "    print(f\"\\nLendo {len(all_csv_paths)} arquivo(s) CSV do COSIF {tipo_fonte}...\")\n",
    "    for csv_path in sorted(list(set(all_csv_paths))):\n",
    "        try:\n",
    "            temp_df = pd.read_csv(csv_path, header=3, encoding='latin1', sep=';', decimal=',', dtype={'CNPJ': str}, on_bad_lines='warn')\n",
    "            temp_df['DATA'] = csv_path.parent.name\n",
    "            df_list.append(temp_df)\n",
    "        except Exception as e:\n",
    "            print(f\"    -> Erro ao ler o arquivo {csv_path.name}: {e}\")\n",
    "\n",
    "    # --- 3. Processamento e Limpeza ---\n",
    "    if not df_list:\n",
    "        print(f\"\\nAVISO: Nenhum dado do COSIF {tipo_fonte} foi carregado.\")\n",
    "        # Cria um DataFrame vazio global para não quebrar as células seguintes\n",
    "        globals()[f'df_cosif_{tipo_fonte.lower()}'] = pd.DataFrame()\n",
    "        return\n",
    "\n",
    "    df_final = pd.concat(df_list, ignore_index=True)\n",
    "    df_final['CNPJ_8'] = standardize_cnpj_base8(df_final['CNPJ'])\n",
    "    df_final['DATA'] = pd.to_datetime(df_final['DATA'], format='%Y%m').dt.strftime('%Y%m').astype(int)\n",
    "\n",
    "    rename_map = {\n",
    "        'NOME_INSTITUICAO': 'NOME_INSTITUICAO_COSIF', 'CONTA': 'CONTA_COSIF',\n",
    "        'NOME_CONTA': 'NOME_CONTA_COSIF', 'SALDO': 'VALOR_CONTA_COSIF',\n",
    "        'COD_CONGL': 'COD_CONGL_PRUD_COSIF', 'NOME_CONGL': 'NOME_CONGL_PRUD_COSIF',\n",
    "        'TAXONOMIA': 'TAXONOMIA_COSIF', 'DOCUMENTO': 'DOCUMENTO_COSIF',\n",
    "        'AGENCIA': 'AGENCIA_COSIF'\n",
    "    }\n",
    "    df_final.rename(columns=rename_map, inplace=True)\n",
    "\n",
    "    for col in ['NOME_INSTITUICAO_COSIF', 'NOME_CONTA_COSIF', 'TAXONOMIA_COSIF', 'NOME_CONGL_PRUD_COSIF']:\n",
    "        if col in df_final.columns:\n",
    "            df_final[col] = clean_text_column(df_final[col])\n",
    "\n",
    "    df_final.drop(columns=[c for c in ['CNPJ', '#DATA_BASE'] if c in df_final.columns], inplace=True)\n",
    "    \n",
    "    cols_ordem = [\n",
    "        'DATA', 'CNPJ_8', 'NOME_INSTITUICAO_COSIF', 'CONTA_COSIF', 'NOME_CONTA_COSIF', \n",
    "        'VALOR_CONTA_COSIF', 'COD_CONGL_PRUD_COSIF', 'NOME_CONGL_PRUD_COSIF'\n",
    "    ]\n",
    "    cols_existentes = [c for c in cols_ordem if c in df_final.columns]\n",
    "    cols_restantes = [c for c in df_final.columns if c not in cols_existentes]\n",
    "    df_final = df_final[cols_existentes + cols_restantes]\n",
    "    \n",
    "    # Disponibiliza o DataFrame no escopo global do notebook\n",
    "    globals()[f'df_cosif_{tipo_fonte.lower()}'] = df_final\n",
    "\n",
    "    print(f\"\\n--- Informações do DataFrame Final: df_cosif_{tipo_fonte.lower()} ---\")\n",
    "    print(f\"Shape: {df_final.shape}\")\n",
    "    print(f\"Período: {df_final['DATA'].min()} a {df_final['DATA'].max()}\")\n",
    "\n",
    "    # --- 4. Salvando o Resultado ---\n",
    "    output_path = dir_output_base / f'df_cosif_{tipo_fonte.lower()}.parquet'\n",
    "    df_final.to_parquet(output_path, index=False)\n",
    "    print(f\"\\nArquivo salvo com sucesso em: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43020058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CÉLULA 2 e 3: EXECUÇÃO DOS PIPELINES COSIF\n",
    "# ==============================================================================\n",
    "# Descrição: Executa os pipelines de ETL para as fontes COSIF Individual\n",
    "# e Prudencial chamando a função genérica 'pipeline_cosif'.\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Execução para COSIF Individual ---\n",
    "pipeline_cosif(\n",
    "    tipo_fonte='Individual',\n",
    "    url_path_segment='Bancos',\n",
    "    file_name_pattern='BANCOS',\n",
    "    sufixos=['BANCOS.csv.zip', 'BANCOS.zip' , 'BANCOS.csv'],\n",
    "    datas=date_range_monthly,\n",
    "    dir_input_base=dir_inputs,\n",
    "    dir_output_base=dir_outputs\n",
    ")\n",
    "\n",
    "# --- Execução para COSIF Prudencial ---\n",
    "pipeline_cosif(\n",
    "    tipo_fonte='Prudencial',\n",
    "    url_path_segment='Conglomerados-prudenciais',\n",
    "    file_name_pattern='BLOPRUDENCIAL',\n",
    "    sufixos=['BLOPRUDENCIAL.csv.zip', 'BLOPRUDENCIAL.zip', 'BLOPRUDENCIAL.csv'],\n",
    "    datas=date_range_monthly,\n",
    "    dir_input_base=dir_inputs,\n",
    "    dir_output_base=dir_outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f67b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CÉLULA 4: Pipeline de Dados - IFDATA Valores\n",
    "# ==============================================================================\n",
    "# Descrição: Baixa e consolida os dados trimestrais de \"Valores\" da API IFDATA.\n",
    "# O resultado final é um único DataFrame salvo em formato Parquet com nomes padronizados.\n",
    "# ==============================================================================\n",
    "print(\"\\n>>> INICIANDO PIPELINE: IFDATA VALORES (PODE DEMORAR) <<<\")\n",
    "\n",
    "# --- 1. Download dos Arquivos ---\n",
    "ifdata_dir = dir_inputs / 'IFDATA'\n",
    "ifdata_dir.mkdir(parents=True, exist_ok=True)\n",
    "tipos_instituicao = [1, 2, 3]\n",
    "\n",
    "MAX_TENTATIVAS = 3\n",
    "DELAY_SEGUNDOS = 5\n",
    "\n",
    "for date_q in date_range_quarterly:\n",
    "    for tipo in tipos_instituicao:\n",
    "        output_csv_val = ifdata_dir / f\"IfDataValores_{date_q}_{tipo}.csv\"\n",
    "        if output_csv_val.exists():\n",
    "            print(f\"IFDATA Valores ({date_q} | Tipo {tipo}): CSV já existe, pulando download.\")\n",
    "            continue\n",
    "\n",
    "        url_val = (\n",
    "            f\"https://olinda.bcb.gov.br/olinda/servico/IFDATA/versao/v1/odata/\"\n",
    "            f\"IfDataValores(AnoMes=@AnoMes,TipoInstituicao=@TipoInstituicao,\"\n",
    "            f\"Relatorio=@Relatorio)?@AnoMes={date_q}&@TipoInstituicao={tipo}\"\n",
    "            f\"&@Relatorio='T'&$format=text/csv\"\n",
    "        )\n",
    "        print(f\"IFDATA Valores ({date_q} | Tipo {tipo}): Baixando…\")\n",
    "\n",
    "        sucesso = False\n",
    "        for tentativa in range(1, MAX_TENTATIVAS + 1):\n",
    "            try:\n",
    "                resp = requests.get(url_val, timeout=120)\n",
    "                resp.raise_for_status()\n",
    "                with open(output_csv_val, 'wb') as f:\n",
    "                    f.write(resp.content)\n",
    "                print(f\"  -> Sucesso na tentativa {tentativa}!\")\n",
    "                sucesso = True\n",
    "                break\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"  -> Tentativa {tentativa} falhou: {e}\")\n",
    "                if tentativa < MAX_TENTATIVAS:\n",
    "                    print(f\"     esperando {DELAY_SEGUNDOS}s antes da próxima tentativa…\")\n",
    "                    time.sleep(DELAY_SEGUNDOS)\n",
    "\n",
    "        if not sucesso:\n",
    "            print(f\"  => Todas as {MAX_TENTATIVAS} tentativas falharam para ({date_q} | Tipo {tipo}). Pulando.\")\n",
    "\n",
    "# --- 2. Leitura e Consolidação dos CSVs ---\n",
    "csv_files = sorted(ifdata_dir.glob(\"IfDataValores_*.csv\"))\n",
    "df_list = []\n",
    "print(f\"\\nLendo {len(csv_files)} arquivo(s) CSV do IFDATA Valores...\")\n",
    "\n",
    "for path in csv_files:\n",
    "    if path.stat().st_size > 100:\n",
    "        print(f\"  Lendo: {path.name}\")\n",
    "        try:\n",
    "            temp = pd.read_csv(path, encoding='utf-8', sep=',', decimal='.', dtype={'CodInst': str})\n",
    "            df_list.append(temp)\n",
    "        except Exception as e:\n",
    "            print(f\"    -> Erro ao ler o arquivo {path.name}: {e}\")\n",
    "\n",
    "# --- 3. Processamento e Limpeza do DataFrame ---\n",
    "if not df_list:\n",
    "    print(\"\\nAVISO: Nenhum dado do IFDATA Valores foi carregado. O DataFrame final estará vazio.\")\n",
    "    df_ifdata_valores = pd.DataFrame()\n",
    "else:\n",
    "    df_ifdata_valores = pd.concat(df_list, ignore_index=True)\n",
    "    print(f\"\\nShape do DF consolidado (bruto): {df_ifdata_valores.shape}\")\n",
    "\n",
    "    # Mapa de renomeação para o novo padrão\n",
    "    rename_map = {\n",
    "        'AnoMes': 'DATA',\n",
    "        'CodInst': 'COD_INST_IFD_VAL',\n",
    "        'TipoInstituicao': 'TIPO_INSTITUICAO_IFD_VAL',\n",
    "        'Conta': 'CONTA_IFD_VAL',\n",
    "        'NomeColuna': 'NOME_CONTA_IFD_VAL',\n",
    "        'Saldo': 'VALOR_CONTA_IFD_VAL',\n",
    "        'NomeRelatorio': 'NOME_RELATORIO_IFD_VAL',\n",
    "        'NumeroRelatorio': 'NUMERO_RELATORIO_IFD_VAL',\n",
    "        'Grupo': 'GRUPO_CONTA_IFD_VAL',\n",
    "        'DescricaoColuna': 'DESCRICAO_CONTA_IFD_VAL'\n",
    "    }\n",
    "    df_ifdata_valores.rename(columns=rename_map, inplace=True)\n",
    "    \n",
    "    # Conversão e limpeza de tipos de dados\n",
    "    df_ifdata_valores['DATA'] = df_ifdata_valores['DATA'].astype(int)\n",
    "    if 'VALOR_CONTA_IFD_VAL' in df_ifdata_valores.columns:\n",
    "        df_ifdata_valores['VALOR_CONTA_IFD_VAL'] = pd.to_numeric(\n",
    "            df_ifdata_valores['VALOR_CONTA_IFD_VAL'].astype(str).str.replace(',', '.'), \n",
    "            errors='coerce'\n",
    "        )\n",
    "\n",
    "    # Limpeza das colunas de texto (já com os novos nomes)\n",
    "    for col in ['NOME_RELATORIO_IFD_VAL', 'GRUPO_CONTA_IFD_VAL', 'NOME_CONTA_IFD_VAL', 'DESCRICAO_CONTA_IFD_VAL']:\n",
    "        if col in df_ifdata_valores.columns:\n",
    "            df_ifdata_valores[col] = clean_text_column(df_ifdata_valores[col])\n",
    "    \n",
    "    # Reordenar colunas\n",
    "    cols_ordem_prioritaria = [\n",
    "        'DATA', 'COD_INST_IFD_VAL', 'TIPO_INSTITUICAO_IFD_VAL', 'CONTA_IFD_VAL', 'NOME_CONTA_IFD_VAL', \n",
    "        'VALOR_CONTA_IFD_VAL', 'NOME_RELATORIO_IFD_VAL', 'NUMERO_RELATORIO_IFD_VAL', \n",
    "        'GRUPO_CONTA_IFD_VAL', 'DESCRICAO_CONTA_IFD_VAL'\n",
    "    ]\n",
    "    cols_existentes_prioritarias = [c for c in cols_ordem_prioritaria if c in df_ifdata_valores.columns]\n",
    "    cols_restantes = [c for c in df_ifdata_valores.columns if c not in cols_existentes_prioritarias]\n",
    "    df_ifdata_valores = df_ifdata_valores[cols_existentes_prioritarias + cols_restantes]\n",
    "\n",
    "    print(\"\\n--- Informações do DataFrame Final: df_ifdata_valores ---\")\n",
    "    print(f\"Shape: {df_ifdata_valores.shape}\")\n",
    "    print(f\"Colunas: {df_ifdata_valores.columns.tolist()}\")\n",
    "    print(f\"Período de dados: {df_ifdata_valores['DATA'].min()} a {df_ifdata_valores['DATA'].max()}\")\n",
    "\n",
    "    # --- 4. Salvando o Resultado ---\n",
    "    output_path = dir_outputs / 'df_ifdata_valores.parquet'\n",
    "    df_ifdata_valores.to_parquet(output_path, index=False)\n",
    "    print(f\"\\nArquivo salvo com sucesso em: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d5a616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CÉLULA 5: Pipeline de Dados - IFDATA Cadastro\n",
    "# ==============================================================================\n",
    "print(\"\\n>>> INICIANDO PIPELINE: IFDATA CADASTRO <<<\")\n",
    "\n",
    "# --- 1. Download dos Arquivos (COM LÓGICA DE RETENTATIVAS) ---\n",
    "ifdata_dir = dir_inputs / 'IFDATA' \n",
    "MAX_TENTATIVAS = 3\n",
    "DELAY_SEGUNDOS = 5\n",
    "\n",
    "for date_m in date_range_monthly:\n",
    "    output_csv_cad = ifdata_dir / f\"IfDataCadastro_{date_m}.csv\"\n",
    "    if output_csv_cad.exists():\n",
    "        print(f\"IFDATA Cadastro ({date_m}): CSV já existe, pulando download.\")\n",
    "        continue\n",
    "            \n",
    "    url_cadastro = f\"https://olinda.bcb.gov.br/olinda/servico/IFDATA/versao/v1/odata/IfDataCadastro(AnoMes=@AnoMes)?@AnoMes={date_m}&$format=text/csv\"\n",
    "    print(f\"IFDATA Cadastro ({date_m}): Baixando...\")\n",
    "    \n",
    "    sucesso = False\n",
    "    for tentativa in range(1, MAX_TENTATIVAS + 1):\n",
    "        try:\n",
    "            resp = requests.get(url_cadastro, timeout=120)\n",
    "            resp.raise_for_status()\n",
    "            with open(output_csv_cad, 'wb') as f:\n",
    "                f.write(resp.content)\n",
    "            print(f\"  -> Sucesso na tentativa {tentativa}!\")\n",
    "            sucesso = True\n",
    "            break\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"  -> Tentativa {tentativa} falhou: {e}\")\n",
    "            if tentativa < MAX_TENTATIVAS:\n",
    "                print(f\"     esperando {DELAY_SEGUNDOS}s antes da próxima tentativa…\")\n",
    "                time.sleep(DELAY_SEGUNDOS)\n",
    "    \n",
    "    if not sucesso:\n",
    "        print(f\"  => Todas as {MAX_TENTATIVAS} tentativas falharam para ({date_m}). Pulando.\")\n",
    "\n",
    "# --- 2. Leitura e Consolidação dos CSVs ---\n",
    "csv_files = sorted(ifdata_dir.glob(\"IfDataCadastro_*.csv\"))\n",
    "df_list = []\n",
    "print(f\"\\nLendo {len(csv_files)} arquivo(s) CSV do IFDATA Cadastro...\")\n",
    "for path in csv_files:\n",
    "    if path.stat().st_size > 100:\n",
    "        print(f\"  Lendo: {path.name}\")\n",
    "        try:\n",
    "            temp = pd.read_csv(path, encoding='utf-8', sep=',', decimal='.', dtype={'CodInst': str, 'CnpjInstituicaoLider': str})\n",
    "            df_list.append(temp)\n",
    "        except Exception as e:\n",
    "            print(f\"    -> Erro ao ler o arquivo {path.name}: {e}\")\n",
    "\n",
    "# --- 3. Processamento e Limpeza do DataFrame ---\n",
    "if not df_list:\n",
    "    print(\"\\nAVISO: Nenhum dado do IFDATA Cadastro foi carregado. O DataFrame final estará vazio.\")\n",
    "    df_ifdata_cadastro = pd.DataFrame()\n",
    "else:\n",
    "    df_ifdata_cadastro = pd.concat(df_list, ignore_index=True).drop_duplicates()\n",
    "    print(f\"\\nShape do DF consolidado (bruto): {df_ifdata_cadastro.shape}\")\n",
    "\n",
    "    # Padronização das chaves\n",
    "    df_ifdata_cadastro['CNPJ_8'] = standardize_cnpj_base8(df_ifdata_cadastro['CodInst'])\n",
    "    df_ifdata_cadastro['CNPJ_LIDER_8_IFD_CAD'] = standardize_cnpj_base8(df_ifdata_cadastro['CnpjInstituicaoLider'])\n",
    "\n",
    "    # Renomeação inicial e padronização de data\n",
    "    df_ifdata_cadastro.rename(columns={'Data': 'DATA'}, inplace=True)\n",
    "    df_ifdata_cadastro['DATA'] = df_ifdata_cadastro['DATA'].astype(int)\n",
    "\n",
    "    # Renomear TODAS as colunas restantes com o sufixo _IFD_CAD\n",
    "    cols_to_rename = [col for col in df_ifdata_cadastro.columns if col not in ['DATA', 'CNPJ_8', 'CNPJ_LIDER_8_IFD_CAD', 'CodInst', 'CnpjInstituicaoLider']]\n",
    "    rename_map = {col: f\"{col.upper()}_IFD_CAD\" for col in cols_to_rename}\n",
    "    \n",
    "    # Renomear as colunas principais do cadastro para o padrão\n",
    "    rename_map.update({\n",
    "        'NomeInstituicao': 'NOME_INSTITUICAO_IFD_CAD',\n",
    "        'CodConglomeradoPrudencial': 'COD_CONGL_PRUD_IFD_CAD',\n",
    "        'CodConglomeradoFinanceiro': 'COD_CONGL_FIN_IFD_CAD',\n",
    "        'DataInicioAtividade': 'DATA_INICIO_ATIVIDADE_IFD_CAD',\n",
    "    })\n",
    "    \n",
    "    df_ifdata_cadastro.rename(columns=rename_map, inplace=True)\n",
    "\n",
    "    # Limpeza de colunas de texto (já com os novos nomes)\n",
    "    text_cols = ['NOME_INSTITUICAO_IFD_CAD', 'SEGMENTOTB_IFD_CAD', 'ATIVIDADE_IFD_CAD', 'UF_IFD_CAD', 'MUNICIPIO_IFD_CAD', 'SITUACAO_IFD_CAD']\n",
    "    for col in text_cols:\n",
    "        if col in df_ifdata_cadastro.columns:\n",
    "            df_ifdata_cadastro[col] = clean_text_column(df_ifdata_cadastro[col])\n",
    "            \n",
    "    # Descarte de colunas redundantes\n",
    "    cols_to_drop = ['CodInst', 'CnpjInstituicaoLider']\n",
    "    df_ifdata_cadastro.drop(columns=[c for c in cols_to_drop if c in df_ifdata_cadastro.columns], inplace=True)\n",
    "\n",
    "    # Reordenar colunas\n",
    "    cols_ordem_prioritaria = [\n",
    "        'DATA', 'CNPJ_8', 'NOME_INSTITUICAO_IFD_CAD', 'COD_CONGL_PRUD_IFD_CAD',\n",
    "        'CNPJ_LIDER_8_IFD_CAD', 'COD_CONGL_FIN_IFD_CAD', 'SITUACAO_IFD_CAD',\n",
    "        'DATA_INICIO_ATIVIDADE_IFD_CAD', 'SEGMENTOTB_IFD_CAD', 'ATIVIDADE_IFD_CAD',\n",
    "        'TCB_IFD_CAD', 'TD_IFD_CAD', 'TC_IFD_CAD', 'UF_IFD_CAD', 'MUNICIPIO_IFD_CAD', 'SR_IFD_CAD'\n",
    "    ]\n",
    "    cols_existentes_prioritarias = [c for c in cols_ordem_prioritaria if c in df_ifdata_cadastro.columns]\n",
    "    cols_restantes = [c for c in df_ifdata_cadastro.columns if c not in cols_existentes_prioritarias]\n",
    "    df_ifdata_cadastro = df_ifdata_cadastro[cols_existentes_prioritarias + cols_restantes]\n",
    "\n",
    "    print(\"\\n--- Informações do DataFrame Final: df_ifdata_cadastro ---\")\n",
    "    print(f\"Shape: {df_ifdata_cadastro.shape}\")\n",
    "    print(f\"Colunas: {df_ifdata_cadastro.columns.tolist()}\")\n",
    "    print(f\"Período de dados: {df_ifdata_cadastro['DATA'].min()} a {df_ifdata_cadastro['DATA'].max()}\")\n",
    "    \n",
    "    # --- 4. Salvando o Resultado ---\n",
    "    output_path = dir_outputs / 'df_ifdata_cadastro.parquet'\n",
    "    df_ifdata_cadastro.to_parquet(output_path, index=False)\n",
    "    print(f\"\\nArquivo salvo com sucesso em: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d59d404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CÉLULA 6: Geração de Saídas Auxiliares (Mapeamento, Infos e Dicionários)\n",
    "# ==============================================================================\n",
    "# Descrição: Cria arquivos secundários úteis para a análise, já adaptados\n",
    "# para a nova estrutura de nomes de colunas e com dicionários COSIF separados.\n",
    "# ==============================================================================\n",
    "print(\"\\n>>> GERANDO SAÍDAS AUXILIARES <<<\")\n",
    "\n",
    "# --- 1. Criar DataFrame de Mapeamento (CNPJ -> Conglomerado) ---\n",
    "try:\n",
    "    if not df_cosif_prudencial.empty:\n",
    "        print(\"\\nCriando mapeamento CNPJ -> Conglomerado...\")\n",
    "        df_mapeamento = (\n",
    "            df_cosif_prudencial[['CNPJ_8', 'COD_CONGL_PRUD_COSIF', 'NOME_CONGL_PRUD_COSIF', 'DATA']]\n",
    "            .sort_values('DATA', ascending=False)\n",
    "            .drop_duplicates(subset=['CNPJ_8'], keep='first')\n",
    "            .drop(columns='DATA')\n",
    "            .reset_index(drop=True)\n",
    "            .rename(columns={\n",
    "                'COD_CONGL_PRUD_COSIF': 'COD_CONGL_PRUD',\n",
    "                'NOME_CONGL_PRUD_COSIF': 'NOME_CONGL_PRUD'\n",
    "            })\n",
    "        )\n",
    "        \n",
    "        output_path_map = dir_outputs / 'df_mapeamento_cnpj_conglomerado.parquet'\n",
    "        df_mapeamento.to_parquet(output_path_map, index=False)\n",
    "        print(f\"Shape do mapeamento: {df_mapeamento.shape}\")\n",
    "        print(f\" -> Sucesso! Arquivo de mapeamento salvo em: {output_path_map}\")\n",
    "    else:\n",
    "        print(\"\\nAVISO: df_cosif_prudencial está vazio. O arquivo de mapeamento não será gerado.\")\n",
    "except NameError:\n",
    "    print(\"\\nAVISO: Variável 'df_cosif_prudencial' não foi criada. O arquivo de mapeamento não será gerado.\")\n",
    "\n",
    "\n",
    "# --- 2. Gerar Arquivos de Informações dos DataFrames (Info) ---\n",
    "print(\"\\n--- Gerando Arquivos de Informações (Perfil) dos DataFrames ---\")\n",
    "dfs_to_document = {\n",
    "    'df_cosif_individual': 'cosif_individual',\n",
    "    'df_cosif_prudencial': 'cosif_prudencial',\n",
    "    'df_ifdata_valores': 'ifdata_valores',\n",
    "    'df_ifdata_cadastro': 'ifdata_cadastro'\n",
    "}\n",
    "\n",
    "for df_var_name, filename_prefix in dfs_to_document.items():\n",
    "    if df_var_name in locals() and not locals()[df_var_name].empty:\n",
    "        df_to_process = locals()[df_var_name]\n",
    "        info_df = create_data_infos(df_to_process, filename_prefix)\n",
    "        \n",
    "        output_excel_path = dir_outputs / f'info_dataframe_{filename_prefix}.xlsx'\n",
    "        info_df.to_excel(output_excel_path, index=False, engine='openpyxl')\n",
    "        print(f\" -> Sucesso! Arquivo de info salvo em: {output_excel_path}\\n\")\n",
    "    else:\n",
    "        print(f\"\\nAVISO: DataFrame '{df_var_name}' está vazio ou não existe. Arquivo de info não será gerado.\")\n",
    "\n",
    "\n",
    "# --- 3. Gerar Dicionários de Contas (Código -> Nome) ---\n",
    "print(\"\\n--- Gerando Dicionários de Contas ---\")\n",
    "\n",
    "# MODIFICAÇÃO: Dicionários COSIF agora são gerados separadamente.\n",
    "\n",
    "# Dicionário para COSIF Individual\n",
    "print(\"\\nProcessando dicionário para: COSIF Individual\")\n",
    "try:\n",
    "    if not df_cosif_individual.empty:\n",
    "        dict_cosif_ind = (\n",
    "            df_cosif_individual[['CONTA_COSIF', 'NOME_CONTA_COSIF']]\n",
    "            .drop_duplicates().sort_values('CONTA_COSIF').reset_index(drop=True)\n",
    "        )\n",
    "        path = dir_outputs / 'dicionario_contas_cosif_individual.xlsx'\n",
    "        dict_cosif_ind.to_excel(path, index=False)\n",
    "        print(f\" -> Sucesso! Dicionário salvo em: {path}\")\n",
    "    else:\n",
    "        print(\" -> AVISO: df_cosif_individual vazio ou colunas de conta não encontradas.\")\n",
    "except NameError:\n",
    "    print(\" -> AVISO: DataFrame 'df_cosif_individual' não existe. Dicionário não gerado.\")\n",
    "\n",
    "# Dicionário para COSIF Prudencial\n",
    "print(\"\\nProcessando dicionário para: COSIF Prudencial\")\n",
    "try:\n",
    "    if not df_cosif_prudencial.empty:\n",
    "        dict_cosif_prud = (\n",
    "            df_cosif_prudencial[['CONTA_COSIF', 'NOME_CONTA_COSIF']]\n",
    "            .drop_duplicates().sort_values('CONTA_COSIF').reset_index(drop=True)\n",
    "        )\n",
    "        path = dir_outputs / 'dicionario_contas_cosif_prudencial.xlsx'\n",
    "        dict_cosif_prud.to_excel(path, index=False)\n",
    "        print(f\" -> Sucesso! Dicionário salvo em: {path}\")\n",
    "    else:\n",
    "        print(\" -> AVISO: df_cosif_prudencial vazio ou colunas de conta não encontradas.\")\n",
    "except NameError:\n",
    "    print(\" -> AVISO: DataFrame 'df_cosif_prudencial' não existe. Dicionário não gerado.\")\n",
    "\n",
    "# Dicionário para IFDATA Valores\n",
    "print(\"\\nProcessando dicionário para: IFDATA Valores\")\n",
    "try:\n",
    "    if not df_ifdata_valores.empty:\n",
    "        dict_ifdata = (\n",
    "            df_ifdata_valores[['CONTA_IFD_VAL', 'NOME_CONTA_IFD_VAL']]\n",
    "            .drop_duplicates().sort_values('CONTA_IFD_VAL').reset_index(drop=True)\n",
    "        )\n",
    "        path = dir_outputs / 'dicionario_contas_ifdata_valores.xlsx'\n",
    "        dict_ifdata.to_excel(path, index=False)\n",
    "        print(f\" -> Sucesso! Dicionário salvo em: {path}\")\n",
    "    else:\n",
    "        print(\" -> AVISO: df_ifdata_valores vazio ou colunas de conta não encontradas.\")\n",
    "except NameError:\n",
    "    print(\" -> AVISO: DataFrame 'df_ifdata_valores' não existe. Dicionário não gerado.\")\n",
    "\n",
    "# --- 4. Gerar Dicionário de Entidades (Nomes, CNPJs e Códigos) ---\n",
    "print(\"\\n--- Gerando Dicionário de Entidades para Consulta ---\")\n",
    "\n",
    "try:\n",
    "    # Passo 1: Criar a base de dados de entidades a partir do IFDATA Cadastro, incluindo todos os identificadores\n",
    "    # Selecionamos a entrada mais recente para cada CNPJ_8 para obter os códigos mais atuais.\n",
    "    df_base_entidades = (\n",
    "        df_ifdata_cadastro[[\n",
    "            'DATA', 'CNPJ_8', 'NOME_INSTITUICAO_IFD_CAD', \n",
    "            'COD_CONGL_PRUD_IFD_CAD', 'COD_CONGL_FIN_IFD_CAD', 'CNPJ_LIDER_8_IFD_CAD'\n",
    "        ]]\n",
    "        .sort_values('DATA', ascending=False)\n",
    "        .drop_duplicates(subset=['CNPJ_8'], keep='first')\n",
    "        .drop(columns='DATA')\n",
    "        .rename(columns={\n",
    "            'NOME_INSTITUICAO_IFD_CAD': 'NOME_PRINCIPAL',\n",
    "            'COD_CONGL_PRUD_IFD_CAD': 'COD_CONGL_PRUD',\n",
    "            'COD_CONGL_FIN_IFD_CAD': 'COD_CONGL_FIN',\n",
    "            'CNPJ_LIDER_8_IFD_CAD': 'CNPJ_LIDER'\n",
    "        })\n",
    "    ).dropna(subset=['CNPJ_8'])\n",
    "\n",
    "    # Passo 2: Coletar todos os nomes únicos e seus CNPJs de todas as fontes\n",
    "    nomes_cosif_prud = df_cosif_prudencial[['CNPJ_8', 'NOME_INSTITUICAO_COSIF']].rename(columns={'NOME_INSTITUICAO_COSIF': 'NOME_VARIACOES'})\n",
    "    nomes_cosif_ind = df_cosif_individual[['CNPJ_8', 'NOME_INSTITUICAO_COSIF']].rename(columns={'NOME_INSTITUICAO_COSIF': 'NOME_VARIACOES'})\n",
    "    nomes_ifd_cad = df_ifdata_cadastro[['CNPJ_8', 'NOME_INSTITUICAO_IFD_CAD']].rename(columns={'NOME_INSTITUICAO_IFD_CAD': 'NOME_VARIACOES'})\n",
    "    \n",
    "    df_todos_nomes = (\n",
    "        pd.concat([nomes_cosif_prud, nomes_cosif_ind, nomes_ifd_cad])\n",
    "        .dropna()\n",
    "        .drop_duplicates()\n",
    "        .groupby('CNPJ_8')['NOME_VARIACOES']\n",
    "        .apply(lambda x: ' | '.join(sorted(list(set(x)))))\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Passo 3: Fazer o merge da base de entidades com a lista de todos os nomes\n",
    "    df_entidades_final = pd.merge(\n",
    "        df_base_entidades,\n",
    "        df_todos_nomes,\n",
    "        on='CNPJ_8',\n",
    "        how='outer'\n",
    "    )\n",
    "    \n",
    "    # Preencher o nome principal caso ele esteja faltando (para entidades que só existem no COSIF)\n",
    "    df_entidades_final['NOME_PRINCIPAL'] = df_entidades_final['NOME_PRINCIPAL'].fillna(\n",
    "        df_entidades_final['NOME_VARIACOES'].str.split(' | ').str[0]\n",
    "    )\n",
    "    \n",
    "    # Reordenar as colunas para uma apresentação lógica\n",
    "    cols_ordem = [\n",
    "        'NOME_PRINCIPAL', \n",
    "        'CNPJ_8', \n",
    "        'CNPJ_LIDER', \n",
    "        'COD_CONGL_PRUD', \n",
    "        'COD_CONGL_FIN', \n",
    "        'NOME_VARIACOES'\n",
    "    ]\n",
    "    df_entidades_final = df_entidades_final[cols_ordem]\n",
    "    df_entidades_final = df_entidades_final.sort_values('NOME_PRINCIPAL').reset_index(drop=True)\n",
    "\n",
    "    # Salvar em Excel\n",
    "    output_path_entidades = dir_outputs / 'dicionario_entidades.xlsx'\n",
    "    df_entidades_final.to_excel(output_path_entidades, index=False)\n",
    "    \n",
    "    print(f\"-> {len(df_entidades_final)} entidades únicas salvas com sucesso em: {output_path_entidades}\")\n",
    "    print(\"O dicionário agora contém todos os identificadores chave (Líder, Prudencial, Financeiro).\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERRO ao gerar o dicionário de entidades: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
