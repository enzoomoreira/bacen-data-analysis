{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ea54b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CÉLULA 1: Configuração do Ambiente e Funções Auxiliares\n",
    "# ==============================================================================\n",
    "# Descrição: Importa bibliotecas, define constantes globais (datas, caminhos),\n",
    "# e cria funções de utilidade que serão usadas ao longo do notebook.\n",
    "# ==============================================================================\n",
    "import pandas as pd\n",
    "import requests\n",
    "import zipfile\n",
    "import warnings\n",
    "import re\n",
    "from pathlib import Path\n",
    "import DataUtils as du\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d33ba97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configurações de Datas e Diretórios ---\n",
    "# Use datas fixas para reprodutibilidade ou ajuste para datas dinâmicas.\n",
    "# Ex: end=pd.Timestamp.now().strftime('%Y-%m-%d')\n",
    "DATE_START = '2024-01-01'\n",
    "DATE_END   = '2024-12-31'\n",
    "\n",
    "date_range_monthly = pd.date_range(start=DATE_START, end=DATE_END, freq='M').strftime(\"%Y%m\").tolist()\n",
    "date_range_quarterly = pd.date_range(start=DATE_START, end=DATE_END, freq='Q').strftime(\"%Y%m\").tolist()\n",
    "\n",
    "# Estrutura de diretórios\n",
    "base_dir    = Path('..').resolve() # O notebook está na raiz do projeto ou em 'Code'\n",
    "dir_inputs  = base_dir / 'Input'\n",
    "dir_outputs = base_dir / 'Output'\n",
    "for d in (dir_inputs, dir_outputs):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Diretório Base: {base_dir}\")\n",
    "print(f\"Diretório de Inputs: {dir_inputs}\")\n",
    "print(f\"Diretório de Outputs: {dir_outputs}\")\n",
    "print(f\"\\nPeríodo de Análise Mensal: de {date_range_monthly[0]} a {date_range_monthly[-1]}\")\n",
    "print(f\"Período de Análise Trimestral: de {date_range_quarterly[0]} a {date_range_quarterly[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0ee1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Funções Auxiliares Locais ---\n",
    "\n",
    "def clean_text_column(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Limpa uma coluna de texto, removendo caracteres de controle e espaços extras.\"\"\"\n",
    "    if series.empty:\n",
    "        return series\n",
    "    \n",
    "    def remove_illegal_chars(text: str) -> str:\n",
    "        if pd.isna(text): return ''\n",
    "        # Remove caracteres de controle ASCII (exceto tab, newline, etc. que já são tratados pelo strip/replace)\n",
    "        return re.sub(r'[\\x00-\\x1F\\x7F]', '', str(text))\n",
    "\n",
    "    return series.astype(str).apply(remove_illegal_chars).str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "\n",
    "def create_data_infos(df: pd.DataFrame, df_name: str) -> pd.DataFrame:\n",
    "    \"\"\"Cria um perfil de dados (info) a partir de um DataFrame.\"\"\"\n",
    "    print(f\"Gerando perfil do DataFrame: '{df_name}'...\")\n",
    "    \n",
    "    dict_df = pd.DataFrame({\n",
    "        'Coluna': df.columns,\n",
    "        'Tipo de Dado (Dtype)': df.dtypes.astype(str),\n",
    "        'Valores Não Nulos': df.count().values,\n",
    "        'Valores Nulos': df.isnull().sum().values,\n",
    "    })\n",
    "    dict_df['% Nulos'] = (dict_df['Valores Nulos'] / len(df) * 100).round(2)\n",
    "    \n",
    "    def get_example(col):\n",
    "        try:\n",
    "            return df[col].dropna().unique()[:3]\n",
    "        except Exception:\n",
    "            return \"Não foi possível extrair exemplos\"\n",
    "            \n",
    "    dict_df['Exemplos'] = [get_example(col) for col in df.columns]\n",
    "    \n",
    "    return dict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47ab0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CÉLULA 2: Pipeline de Dados - COSIF Individual\n",
    "# ==============================================================================\n",
    "# Descrição: Baixa, extrai, lê e consolida os arquivos mensais do\n",
    "# COSIF Individual (Bancos). O resultado final é um único DataFrame\n",
    "# salvo em formato Parquet com nomes de colunas padronizados.\n",
    "# ==============================================================================\n",
    "print(\"\\n>>> INICIANDO PIPELINE: COSIF INDIVIDUAL <<<\")\n",
    "\n",
    "# --- 1. Download e Extração dos Arquivos ---\n",
    "cosif_ind_dir = dir_inputs / 'COSIF' / 'individual'\n",
    "cosif_ind_dir.mkdir(parents=True, exist_ok=True)\n",
    "all_individual_csv_paths = []\n",
    "\n",
    "suffixes = ['BANCOS.csv.zip', 'BANCOS.zip' , 'BANCOS.csv']\n",
    "\n",
    "for date in date_range_monthly:\n",
    "    subfolder = cosif_ind_dir / date\n",
    "    subfolder.mkdir(exist_ok=True)\n",
    "    \n",
    "    existing_csvs = list(subfolder.glob(\"*BANCOS*.csv\"))\n",
    "    if existing_csvs:\n",
    "        print(f\"COSIF Ind. ({date}): CSV já existe, pulando download.\")\n",
    "        all_individual_csv_paths.extend(existing_csvs)\n",
    "        continue\n",
    "\n",
    "    download_success = False\n",
    "    for suffix in suffixes:\n",
    "        if download_success:\n",
    "            break # Se já baixou para esta data, sai do loop de sufixos\n",
    "\n",
    "        local_file = subfolder / f\"{date}{suffix}\"\n",
    "\n",
    "        url = f\"https://www.bcb.gov.br/content/estabilidadefinanceira/cosif/Bancos/{date}{suffix}\"\n",
    "\n",
    "        print(f\"COSIF Ind. ({date}): Tentando baixar de {url}...\")\n",
    "        try:\n",
    "            resp = requests.get(url, timeout=90)\n",
    "            resp.raise_for_status() # Lança erro para status 4xx/5xx\n",
    "\n",
    "            content_type = resp.headers.get('Content-Type', '').lower()\n",
    "            # Validação para garantir que não é uma página de erro HTML\n",
    "            if 'html' in content_type:\n",
    "                print(\"  -> Falha: Recebido conteúdo HTML, pulando para próximo sufixo.\")\n",
    "                continue\n",
    "\n",
    "            # --- Lógica para tratar ZIP ---\n",
    "            if 'zip' in suffix.lower():\n",
    "                with open(local_file, 'wb') as f:\n",
    "                    f.write(resp.content)\n",
    "                \n",
    "                with zipfile.ZipFile(local_file, 'r') as zf:\n",
    "                    extracted = False\n",
    "                    for member in zf.namelist():\n",
    "                        if member.lower().endswith('.csv') and \"bancos\" in member.lower():\n",
    "                            zf.extract(member, subfolder)\n",
    "                            extracted_file = subfolder / member\n",
    "                            all_individual_csv_paths.append(extracted_file)\n",
    "                            print(f\"  -> Sucesso! Arquivo '{member}' extraído de '{local_file.name}'.\")\n",
    "                            extracted = True\n",
    "                    if extracted:\n",
    "                        download_success = True # Marca o sucesso para esta data\n",
    "                    else:\n",
    "                        print(\"  -> Falha: ZIP baixado não contém o CSV esperado.\")\n",
    "\n",
    "            # --- Lógica para tratar CSV direto ---\n",
    "            elif '.csv' in suffix.lower():\n",
    "                # Salva o arquivo CSV diretamente\n",
    "                with open(local_file, 'wb') as f:\n",
    "                    f.write(resp.content)\n",
    "                all_individual_csv_paths.append(local_file)\n",
    "                print(f\"  -> Sucesso! Arquivo CSV '{local_file.name}' baixado diretamente.\")\n",
    "                download_success = True # Marca o sucesso para esta data\n",
    "\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            # Erro comum para arquivo não encontrado (404), não é um erro fatal do script\n",
    "            print(f\"  -> Info: Arquivo não encontrado em {url} (Status: {e.response.status_code}).\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"  -> Erro de conexão ao tentar {url}: {e}\")\n",
    "        except zipfile.BadZipFile:\n",
    "            print(f\"  -> Erro: Arquivo baixado de {url} não é um ZIP válido.\")\n",
    "        except Exception as e:\n",
    "            print(f\"  -> Erro inesperado ao processar {url}: {e}\")\n",
    "\n",
    "    # Se após todas as tentativas o download falhou para a data\n",
    "    if not download_success:\n",
    "        print(f\"COSIF Ind. ({date}): FALHA no download após tentar todos os sufixos.\")\n",
    "\n",
    "# --- 2. Leitura e Consolidação dos CSVs ---\n",
    "df_list = []\n",
    "print(f\"\\nLendo {len(all_individual_csv_paths)} arquivo(s) CSV do COSIF Individual...\")\n",
    "for csv_path in sorted(list(set(all_individual_csv_paths))):\n",
    "    print(f\"  Lendo: {csv_path.name}\")\n",
    "    try:\n",
    "        temp_df = pd.read_csv(csv_path, header=3, encoding='latin1', sep=';', decimal=',', dtype={'CNPJ': str}, on_bad_lines='warn')\n",
    "        file_date = csv_path.parent.name\n",
    "        temp_df['DATA'] = file_date\n",
    "        df_list.append(temp_df)\n",
    "    except Exception as e:\n",
    "        print(f\"    -> Erro ao ler o arquivo {csv_path.name}: {e}\")\n",
    "\n",
    "# --- 3. Processamento e Limpeza do DataFrame ---\n",
    "if not df_list:\n",
    "    print(\"\\nAVISO: Nenhum dado do COSIF Individual foi carregado. O DataFrame final estará vazio.\")\n",
    "    df_cosif_individual = pd.DataFrame()\n",
    "else:\n",
    "    df_cosif_individual = pd.concat(df_list, ignore_index=True)\n",
    "    print(f\"\\nShape do DF consolidado (bruto): {df_cosif_individual.shape}\")\n",
    "\n",
    "    # Padronização da chave CNPJ_8\n",
    "    df_cosif_individual['CNPJ_8'] = du.standardize_cnpj_base8(df_cosif_individual['CNPJ'])\n",
    "    \n",
    "    # Padronização de data\n",
    "    df_cosif_individual['DATA'] = pd.to_datetime(df_cosif_individual['DATA'], format='%Y%m').dt.strftime('%Y%m').astype(int)\n",
    "\n",
    "    # Mapa de renomeação para o novo padrão\n",
    "    rename_map = {\n",
    "        'NOME_INSTITUICAO': 'NOME_INSTITUICAO_COSIF',\n",
    "        'CONTA': 'CONTA_COSIF',\n",
    "        'NOME_CONTA': 'NOME_CONTA_COSIF',\n",
    "        'SALDO': 'VALOR_CONTA_COSIF',\n",
    "        'COD_CONGL': 'COD_CONGL_PRUD_COSIF',\n",
    "        'NOME_CONGL': 'NOME_CONGL_PRUD_COSIF',\n",
    "        'TAXONOMIA': 'TAXONOMIA_COSIF',\n",
    "        'DOCUMENTO': 'DOCUMENTO_COSIF',\n",
    "        'AGENCIA': 'AGENCIA_COSIF'\n",
    "    }\n",
    "    df_cosif_individual.rename(columns=rename_map, inplace=True)\n",
    "\n",
    "    # Limpeza das colunas de texto (já com os novos nomes)\n",
    "    for col in ['NOME_INSTITUICAO_COSIF', 'NOME_CONTA_COSIF', 'TAXONOMIA_COSIF']:\n",
    "        if col in df_cosif_individual.columns:\n",
    "            df_cosif_individual[col] = clean_text_column(df_cosif_individual[col])\n",
    "\n",
    "    # Descarte de colunas redundantes\n",
    "    cols_to_drop = ['CNPJ', '#DATA_BASE']\n",
    "    df_cosif_individual.drop(columns=[c for c in cols_to_drop if c in df_cosif_individual.columns], inplace=True)\n",
    "\n",
    "    # Reordenar colunas para melhor visualização, garantindo que todas as outras sejam mantidas\n",
    "    cols_ordem_prioritaria = [\n",
    "        'DATA', 'CNPJ_8', 'NOME_INSTITUICAO_COSIF', 'CONTA_COSIF', 'NOME_CONTA_COSIF', \n",
    "        'VALOR_CONTA_COSIF', 'COD_CONGL_PRUD_COSIF', 'NOME_CONGL_PRUD_COSIF', \n",
    "        'TAXONOMIA_COSIF', 'DOCUMENTO_COSIF', 'AGENCIA_COSIF'\n",
    "    ]\n",
    "    cols_existentes_prioritarias = [c for c in cols_ordem_prioritaria if c in df_cosif_individual.columns]\n",
    "    cols_restantes = [c for c in df_cosif_individual.columns if c not in cols_existentes_prioritarias]\n",
    "    df_cosif_individual = df_cosif_individual[cols_existentes_prioritarias + cols_restantes]\n",
    "\n",
    "    print(\"\\n--- Informações do DataFrame Final: df_cosif_individual ---\")\n",
    "    print(f\"Shape: {df_cosif_individual.shape}\")\n",
    "    print(f\"Colunas: {df_cosif_individual.columns.tolist()}\")\n",
    "    print(f\"Período de dados: {df_cosif_individual['DATA'].min()} a {df_cosif_individual['DATA'].max()}\")\n",
    "\n",
    "    # --- 4. Salvando o Resultado ---\n",
    "    output_path = dir_outputs / 'df_cosif_individual.parquet'\n",
    "    df_cosif_individual.to_parquet(output_path, index=False)\n",
    "    print(f\"\\nArquivo salvo com sucesso em: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43020058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CÉLULA 3: Pipeline de Dados - COSIF Prudencial\n",
    "# ==============================================================================\n",
    "# Descrição: Baixa, extrai, lê e consolida os arquivos mensais do\n",
    "# COSIF Prudencial (Conglomerados). O resultado final é um único DataFrame\n",
    "# salvo em formato Parquet com nomes de colunas padronizados.\n",
    "# ==============================================================================\n",
    "print(\"\\n>>> INICIANDO PIPELINE: COSIF PRUDENCIAL <<<\")\n",
    "\n",
    "# --- 1. Download e Extração dos Arquivos ---\n",
    "cosif_prud_dir = dir_inputs / 'COSIF' / 'prudencial'\n",
    "cosif_prud_dir.mkdir(parents=True, exist_ok=True)\n",
    "all_prudencial_csv_paths = []\n",
    "\n",
    "suffixes = ['BLOPRUDENCIAL.csv.zip', 'BLOPRUDENCIAL.zip', 'BLOPRUDENCIAL.csv']\n",
    "\n",
    "for date in date_range_monthly:\n",
    "    subfolder = cosif_prud_dir / date\n",
    "    subfolder.mkdir(exist_ok=True)\n",
    "    \n",
    "    existing_csvs = list(subfolder.glob(\"*BLOPRUDENCIAL*.csv\"))\n",
    "    if existing_csvs:\n",
    "        print(f\"COSIF Prud. ({date}): CSV já existe, pulando download.\")\n",
    "        all_prudencial_csv_paths.extend(existing_csvs)\n",
    "        continue\n",
    "\n",
    "    download_success = False\n",
    "    for suffix in suffixes:\n",
    "        if download_success:\n",
    "            break # Se já baixou para esta data, sai do loop de sufixos\n",
    "\n",
    "        local_file = subfolder / f\"{date}{suffix}\"\n",
    "\n",
    "        ### URL base para o COSIF Prudencial\n",
    "        url = f\"https://www.bcb.gov.br/content/estabilidadefinanceira/cosif/Conglomerados-prudenciais/{date}{suffix}\"\n",
    "\n",
    "        print(f\"COSIF Prud. ({date}): Tentando baixar de {url}...\")\n",
    "        try:\n",
    "            resp = requests.get(url, timeout=90)\n",
    "            resp.raise_for_status() # Lança erro para status 4xx/5xx\n",
    "\n",
    "            content_type = resp.headers.get('Content-Type', '').lower()\n",
    "            # Validação para garantir que não é uma página de erro HTML\n",
    "            if 'html' in content_type:\n",
    "                print(\"  -> Falha: Recebido conteúdo HTML, pulando para próximo sufixo.\")\n",
    "                continue\n",
    "\n",
    "            # --- Lógica para tratar ZIP ---\n",
    "            if 'zip' in suffix.lower():\n",
    "                with open(local_file, 'wb') as f:\n",
    "                    f.write(resp.content)\n",
    "                \n",
    "                with zipfile.ZipFile(local_file, 'r') as zf:\n",
    "                    extracted = False\n",
    "                    for member in zf.namelist():\n",
    "                        ### ALTERAÇÃO: Verificação do nome do arquivo interno específica para Prudencial\n",
    "                        if member.lower().endswith('.csv') and \"prudencial\" in member.lower():\n",
    "                            zf.extract(member, subfolder)\n",
    "                            extracted_file = subfolder / member\n",
    "                            all_prudencial_csv_paths.append(extracted_file)\n",
    "                            print(f\"  -> Sucesso! Arquivo '{member}' extraído de '{local_file.name}'.\")\n",
    "                            extracted = True\n",
    "                    if extracted:\n",
    "                        download_success = True # Marca o sucesso para esta data\n",
    "                    else:\n",
    "                        print(\"  -> Falha: ZIP baixado não contém o CSV esperado.\")\n",
    "\n",
    "            # --- Lógica para tratar CSV direto ---\n",
    "            elif '.csv' in suffix.lower():\n",
    "                # Salva o arquivo CSV diretamente\n",
    "                with open(local_file, 'wb') as f:\n",
    "                    f.write(resp.content)\n",
    "                all_prudencial_csv_paths.append(local_file)\n",
    "                print(f\"  -> Sucesso! Arquivo CSV '{local_file.name}' baixado diretamente.\")\n",
    "                download_success = True # Marca o sucesso para esta data\n",
    "\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            # Erro comum para arquivo não encontrado (404), não é um erro fatal do script\n",
    "            print(f\"  -> Info: Arquivo não encontrado em {url} (Status: {e.response.status_code}).\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"  -> Erro de conexão ao tentar {url}: {e}\")\n",
    "        except zipfile.BadZipFile:\n",
    "            print(f\"  -> Erro: Arquivo baixado de {url} não é um ZIP válido.\")\n",
    "        except Exception as e:\n",
    "            print(f\"  -> Erro inesperado ao processar {url}: {e}\")\n",
    "\n",
    "    # Se após todas as tentativas o download falhou para a data\n",
    "    if not download_success:\n",
    "        print(f\"COSIF Prud. ({date}): FALHA no download após tentar todos os sufixos.\")\n",
    "\n",
    "# --- 2. Leitura e Consolidação dos CSVs ---\n",
    "df_list = []\n",
    "print(f\"\\nLendo {len(all_prudencial_csv_paths)} arquivo(s) CSV do COSIF Prudencial...\")\n",
    "for csv_path in sorted(list(set(all_prudencial_csv_paths))):\n",
    "    print(f\"  Lendo: {csv_path.name}\")\n",
    "    try:\n",
    "        temp_df = pd.read_csv(csv_path, header=3, encoding='latin1', sep=';', decimal=',', dtype={'CNPJ': str}, on_bad_lines='warn')\n",
    "        file_date = csv_path.parent.name\n",
    "        temp_df['DATA'] = file_date\n",
    "        df_list.append(temp_df)\n",
    "    except Exception as e:\n",
    "        print(f\"    -> Erro ao ler o arquivo {csv_path.name}: {e}\")\n",
    "        \n",
    "# --- 3. Processamento e Limpeza do DataFrame ---\n",
    "if not df_list:\n",
    "    print(\"\\nAVISO: Nenhum dado do COSIF Prudencial foi carregado. O DataFrame final estará vazio.\")\n",
    "    df_cosif_prudencial = pd.DataFrame()\n",
    "else:\n",
    "    df_cosif_prudencial = pd.concat(df_list, ignore_index=True)\n",
    "    print(f\"\\nShape do DF consolidado (bruto): {df_cosif_prudencial.shape}\")\n",
    "\n",
    "    # Padronização da chave CNPJ_8\n",
    "    df_cosif_prudencial['CNPJ_8'] = du.standardize_cnpj_base8(df_cosif_prudencial['CNPJ'])\n",
    "    \n",
    "    # Padronização de data\n",
    "    df_cosif_prudencial['DATA'] = pd.to_datetime(df_cosif_prudencial['DATA'], format='%Y%m').dt.strftime('%Y%m').astype(int)\n",
    "\n",
    "    # Mapa de renomeação para o novo padrão\n",
    "    rename_map = {\n",
    "        'NOME_INSTITUICAO': 'NOME_INSTITUICAO_COSIF',\n",
    "        'CONTA': 'CONTA_COSIF',\n",
    "        'NOME_CONTA': 'NOME_CONTA_COSIF',\n",
    "        'SALDO': 'VALOR_CONTA_COSIF',\n",
    "        'COD_CONGL': 'COD_CONGL_PRUD_COSIF',\n",
    "        'NOME_CONGL': 'NOME_CONGL_PRUD_COSIF',\n",
    "        'TAXONOMIA': 'TAXONOMIA_COSIF',\n",
    "        'DOCUMENTO': 'DOCUMENTO_COSIF',\n",
    "        'AGENCIA': 'AGENCIA_COSIF'\n",
    "    }\n",
    "    df_cosif_prudencial.rename(columns=rename_map, inplace=True)\n",
    "    \n",
    "    # Limpeza das colunas de texto (já com os novos nomes)\n",
    "    for col in ['NOME_INSTITUICAO_COSIF', 'NOME_CONGL_PRUD_COSIF', 'NOME_CONTA_COSIF']:\n",
    "        if col in df_cosif_prudencial.columns:\n",
    "            df_cosif_prudencial[col] = clean_text_column(df_cosif_prudencial[col])\n",
    "    \n",
    "    # Descarte de colunas redundantes\n",
    "    cols_to_drop = ['CNPJ', '#DATA_BASE']\n",
    "    df_cosif_prudencial.drop(columns=[c for c in cols_to_drop if c in df_cosif_prudencial.columns], inplace=True)\n",
    "\n",
    "    # Reordenar colunas para melhor visualização\n",
    "    cols_ordem_prioritaria = [\n",
    "        'DATA', 'CNPJ_8', 'NOME_INSTITUICAO_COSIF', 'CONTA_COSIF', 'NOME_CONTA_COSIF', \n",
    "        'VALOR_CONTA_COSIF', 'COD_CONGL_PRUD_COSIF', 'NOME_CONGL_PRUD_COSIF', \n",
    "        'TAXONOMIA_COSIF', 'DOCUMENTO_COSIF', 'AGENCIA_COSIF'\n",
    "    ]\n",
    "    cols_existentes_prioritarias = [c for c in cols_ordem_prioritaria if c in df_cosif_prudencial.columns]\n",
    "    cols_restantes = [c for c in df_cosif_prudencial.columns if c not in cols_existentes_prioritarias]\n",
    "    df_cosif_prudencial = df_cosif_prudencial[cols_existentes_prioritarias + cols_restantes]\n",
    "\n",
    "    print(\"\\n--- Informações do DataFrame Final: df_cosif_prudencial ---\")\n",
    "    print(f\"Shape: {df_cosif_prudencial.shape}\")\n",
    "    print(f\"Colunas: {df_cosif_prudencial.columns.tolist()}\")\n",
    "    print(f\"Período de dados: {df_cosif_prudencial['DATA'].min()} a {df_cosif_prudencial['DATA'].max()}\")\n",
    "\n",
    "    # --- 4. Salvando o Resultado ---\n",
    "    output_path = dir_outputs / 'df_cosif_prudencial.parquet'\n",
    "    df_cosif_prudencial.to_parquet(output_path, index=False)\n",
    "    print(f\"\\nArquivo salvo com sucesso em: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f67b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CÉLULA 4: Pipeline de Dados - IFDATA Valores\n",
    "# ==============================================================================\n",
    "# Descrição: Baixa e consolida os dados trimestrais de \"Valores\" da API IFDATA.\n",
    "# O resultado final é um único DataFrame salvo em formato Parquet com nomes padronizados.\n",
    "# ==============================================================================\n",
    "print(\"\\n>>> INICIANDO PIPELINE: IFDATA VALORES (PODE DEMORAR) <<<\")\n",
    "\n",
    "# --- 1. Download dos Arquivos ---\n",
    "ifdata_dir = dir_inputs / 'IFDATA'\n",
    "ifdata_dir.mkdir(parents=True, exist_ok=True)\n",
    "tipos_instituicao = [1, 2, 3]\n",
    "\n",
    "MAX_TENTATIVAS = 3\n",
    "DELAY_SEGUNDOS = 5\n",
    "\n",
    "for date_q in date_range_quarterly:\n",
    "    for tipo in tipos_instituicao:\n",
    "        output_csv_val = ifdata_dir / f\"IfDataValores_{date_q}_{tipo}.csv\"\n",
    "        if output_csv_val.exists():\n",
    "            print(f\"IFDATA Valores ({date_q} | Tipo {tipo}): CSV já existe, pulando download.\")\n",
    "            continue\n",
    "\n",
    "        url_val = (\n",
    "            f\"https://olinda.bcb.gov.br/olinda/servico/IFDATA/versao/v1/odata/\"\n",
    "            f\"IfDataValores(AnoMes=@AnoMes,TipoInstituicao=@TipoInstituicao,\"\n",
    "            f\"Relatorio=@Relatorio)?@AnoMes={date_q}&@TipoInstituicao={tipo}\"\n",
    "            f\"&@Relatorio='T'&$format=text/csv\"\n",
    "        )\n",
    "        print(f\"IFDATA Valores ({date_q} | Tipo {tipo}): Baixando…\")\n",
    "\n",
    "        sucesso = False\n",
    "        for tentativa in range(1, MAX_TENTATIVAS + 1):\n",
    "            try:\n",
    "                resp = requests.get(url_val, timeout=120)\n",
    "                resp.raise_for_status()\n",
    "                with open(output_csv_val, 'wb') as f:\n",
    "                    f.write(resp.content)\n",
    "                print(f\"  -> Sucesso na tentativa {tentativa}!\")\n",
    "                sucesso = True\n",
    "                break\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"  -> Tentativa {tentativa} falhou: {e}\")\n",
    "                if tentativa < MAX_TENTATIVAS:\n",
    "                    print(f\"     esperando {DELAY_SEGUNDOS}s antes da próxima tentativa…\")\n",
    "                    time.sleep(DELAY_SEGUNDOS)\n",
    "\n",
    "        if not sucesso:\n",
    "            print(f\"  => Todas as {MAX_TENTATIVAS} tentativas falharam para ({date_q} | Tipo {tipo}). Pulando.\")\n",
    "\n",
    "# --- 2. Leitura e Consolidação dos CSVs ---\n",
    "csv_files = sorted(ifdata_dir.glob(\"IfDataValores_*.csv\"))\n",
    "df_list = []\n",
    "print(f\"\\nLendo {len(csv_files)} arquivo(s) CSV do IFDATA Valores...\")\n",
    "\n",
    "for path in csv_files:\n",
    "    if path.stat().st_size > 100:\n",
    "        print(f\"  Lendo: {path.name}\")\n",
    "        try:\n",
    "            temp = pd.read_csv(path, encoding='utf-8', sep=',', decimal='.', dtype={'CodInst': str})\n",
    "            df_list.append(temp)\n",
    "        except Exception as e:\n",
    "            print(f\"    -> Erro ao ler o arquivo {path.name}: {e}\")\n",
    "\n",
    "# --- 3. Processamento e Limpeza do DataFrame ---\n",
    "if not df_list:\n",
    "    print(\"\\nAVISO: Nenhum dado do IFDATA Valores foi carregado. O DataFrame final estará vazio.\")\n",
    "    df_ifdata_valores = pd.DataFrame()\n",
    "else:\n",
    "    df_ifdata_valores = pd.concat(df_list, ignore_index=True)\n",
    "    print(f\"\\nShape do DF consolidado (bruto): {df_ifdata_valores.shape}\")\n",
    "\n",
    "    # Mapa de renomeação para o novo padrão\n",
    "    rename_map = {\n",
    "        'AnoMes': 'DATA',\n",
    "        'CodInst': 'COD_INST_IFD_VAL',\n",
    "        'TipoInstituicao': 'TIPO_INSTITUICAO_IFD_VAL',\n",
    "        'Conta': 'CONTA_IFD_VAL',\n",
    "        'NomeColuna': 'NOME_CONTA_IFD_VAL',\n",
    "        'Saldo': 'VALOR_CONTA_IFD_VAL',\n",
    "        'NomeRelatorio': 'NOME_RELATORIO_IFD_VAL',\n",
    "        'NumeroRelatorio': 'NUMERO_RELATORIO_IFD_VAL',\n",
    "        'Grupo': 'GRUPO_CONTA_IFD_VAL',\n",
    "        'DescricaoColuna': 'DESCRICAO_CONTA_IFD_VAL'\n",
    "    }\n",
    "    df_ifdata_valores.rename(columns=rename_map, inplace=True)\n",
    "    \n",
    "    # Conversão e limpeza de tipos de dados\n",
    "    df_ifdata_valores['DATA'] = df_ifdata_valores['DATA'].astype(int)\n",
    "    if 'VALOR_CONTA_IFD_VAL' in df_ifdata_valores.columns:\n",
    "        df_ifdata_valores['VALOR_CONTA_IFD_VAL'] = pd.to_numeric(\n",
    "            df_ifdata_valores['VALOR_CONTA_IFD_VAL'].astype(str).str.replace(',', '.'), \n",
    "            errors='coerce'\n",
    "        )\n",
    "\n",
    "    # Limpeza das colunas de texto (já com os novos nomes)\n",
    "    for col in ['NOME_RELATORIO_IFD_VAL', 'GRUPO_CONTA_IFD_VAL', 'NOME_CONTA_IFD_VAL', 'DESCRICAO_CONTA_IFD_VAL']:\n",
    "        if col in df_ifdata_valores.columns:\n",
    "            df_ifdata_valores[col] = clean_text_column(df_ifdata_valores[col])\n",
    "    \n",
    "    # Reordenar colunas\n",
    "    cols_ordem_prioritaria = [\n",
    "        'DATA', 'COD_INST_IFD_VAL', 'TIPO_INSTITUICAO_IFD_VAL', 'CONTA_IFD_VAL', 'NOME_CONTA_IFD_VAL', \n",
    "        'VALOR_CONTA_IFD_VAL', 'NOME_RELATORIO_IFD_VAL', 'NUMERO_RELATORIO_IFD_VAL', \n",
    "        'GRUPO_CONTA_IFD_VAL', 'DESCRICAO_CONTA_IFD_VAL'\n",
    "    ]\n",
    "    cols_existentes_prioritarias = [c for c in cols_ordem_prioritaria if c in df_ifdata_valores.columns]\n",
    "    cols_restantes = [c for c in df_ifdata_valores.columns if c not in cols_existentes_prioritarias]\n",
    "    df_ifdata_valores = df_ifdata_valores[cols_existentes_prioritarias + cols_restantes]\n",
    "\n",
    "    print(\"\\n--- Informações do DataFrame Final: df_ifdata_valores ---\")\n",
    "    print(f\"Shape: {df_ifdata_valores.shape}\")\n",
    "    print(f\"Colunas: {df_ifdata_valores.columns.tolist()}\")\n",
    "    print(f\"Período de dados: {df_ifdata_valores['DATA'].min()} a {df_ifdata_valores['DATA'].max()}\")\n",
    "\n",
    "    # --- 4. Salvando o Resultado ---\n",
    "    output_path = dir_outputs / 'df_ifdata_valores.parquet'\n",
    "    df_ifdata_valores.to_parquet(output_path, index=False)\n",
    "    print(f\"\\nArquivo salvo com sucesso em: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d5a616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CÉLULA 5: Pipeline de Dados - IFDATA Cadastro\n",
    "# ==============================================================================\n",
    "# Descrição: Baixa e consolida os dados mensais de \"Cadastro\" da API IFDATA.\n",
    "# O resultado final é um único DataFrame salvo em formato Parquet com nomes padronizados.\n",
    "# ==============================================================================\n",
    "print(\"\\n>>> INICIANDO PIPELINE: IFDATA CADASTRO <<<\")\n",
    "\n",
    "# --- 1. Download dos Arquivos ---\n",
    "ifdata_dir = dir_inputs / 'IFDATA' \n",
    "for date_m in date_range_monthly:\n",
    "    output_csv_cad = ifdata_dir / f\"IfDataCadastro_{date_m}.csv\"\n",
    "    if output_csv_cad.exists():\n",
    "        print(f\"IFDATA Cadastro ({date_m}): CSV já existe, pulando download.\")\n",
    "        continue\n",
    "            \n",
    "    url_cadastro = f\"https://olinda.bcb.gov.br/olinda/servico/IFDATA/versao/v1/odata/IfDataCadastro(AnoMes=@AnoMes)?@AnoMes={date_m}&$format=text/csv\"\n",
    "    print(f\"IFDATA Cadastro ({date_m}): Baixando...\")\n",
    "    try:\n",
    "        resp = requests.get(url_cadastro, timeout=120)\n",
    "        resp.raise_for_status()\n",
    "        with open(output_csv_cad, 'wb') as f:\n",
    "            f.write(resp.content)\n",
    "        print(\"  -> Sucesso!\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"  -> Erro no download: {e}\")\n",
    "\n",
    "# --- 2. Leitura e Consolidação dos CSVs ---\n",
    "csv_files = sorted(ifdata_dir.glob(\"IfDataCadastro_*.csv\"))\n",
    "df_list = []\n",
    "print(f\"\\nLendo {len(csv_files)} arquivo(s) CSV do IFDATA Cadastro...\")\n",
    "for path in csv_files:\n",
    "    if path.stat().st_size > 100:\n",
    "        print(f\"  Lendo: {path.name}\")\n",
    "        try:\n",
    "            temp = pd.read_csv(path, encoding='utf-8', sep=',', decimal='.', dtype={'CodInst': str, 'CnpjInstituicaoLider': str})\n",
    "            df_list.append(temp)\n",
    "        except Exception as e:\n",
    "            print(f\"    -> Erro ao ler o arquivo {path.name}: {e}\")\n",
    "\n",
    "# --- 3. Processamento e Limpeza do DataFrame ---\n",
    "if not df_list:\n",
    "    print(\"\\nAVISO: Nenhum dado do IFDATA Cadastro foi carregado. O DataFrame final estará vazio.\")\n",
    "    df_ifdata_cadastro = pd.DataFrame()\n",
    "else:\n",
    "    df_ifdata_cadastro = pd.concat(df_list, ignore_index=True).drop_duplicates()\n",
    "    print(f\"\\nShape do DF consolidado (bruto): {df_ifdata_cadastro.shape}\")\n",
    "\n",
    "    # Padronização das chaves\n",
    "    df_ifdata_cadastro['CNPJ_8'] = du.standardize_cnpj_base8(df_ifdata_cadastro['CodInst'])\n",
    "    df_ifdata_cadastro['CNPJ_LIDER_8_IFD_CAD'] = du.standardize_cnpj_base8(df_ifdata_cadastro['CnpjInstituicaoLider'])\n",
    "\n",
    "    # Renomeação inicial e padronização de data\n",
    "    df_ifdata_cadastro.rename(columns={'Data': 'DATA'}, inplace=True)\n",
    "    df_ifdata_cadastro['DATA'] = df_ifdata_cadastro['DATA'].astype(int)\n",
    "\n",
    "    # Renomear TODAS as colunas restantes com o sufixo _IFD_CAD\n",
    "    cols_to_rename = [col for col in df_ifdata_cadastro.columns if col not in ['DATA', 'CNPJ_8', 'CNPJ_LIDER_8_IFD_CAD', 'CodInst', 'CnpjInstituicaoLider']]\n",
    "    rename_map = {col: f\"{col.upper()}_IFD_CAD\" for col in cols_to_rename}\n",
    "    \n",
    "    # Renomear as colunas principais do cadastro para o padrão\n",
    "    rename_map.update({\n",
    "        'NomeInstituicao': 'NOME_INSTITUICAO_IFD_CAD',\n",
    "        'CodConglomeradoPrudencial': 'COD_CONGL_PRUD_IFD_CAD',\n",
    "        'CodConglomeradoFinanceiro': 'COD_CONGL_FIN_IFD_CAD',\n",
    "        'DataInicioAtividade': 'DATA_INICIO_ATIVIDADE_IFD_CAD',\n",
    "    })\n",
    "    \n",
    "    df_ifdata_cadastro.rename(columns=rename_map, inplace=True)\n",
    "\n",
    "    # Limpeza de colunas de texto (já com os novos nomes)\n",
    "    text_cols = ['NOME_INSTITUICAO_IFD_CAD', 'SEGMENTOTB_IFD_CAD', 'ATIVIDADE_IFD_CAD', 'UF_IFD_CAD', 'MUNICIPIO_IFD_CAD', 'SITUACAO_IFD_CAD']\n",
    "    for col in text_cols:\n",
    "        if col in df_ifdata_cadastro.columns:\n",
    "            df_ifdata_cadastro[col] = clean_text_column(df_ifdata_cadastro[col])\n",
    "            \n",
    "    # Descarte de colunas redundantes\n",
    "    cols_to_drop = ['CodInst', 'CnpjInstituicaoLider']\n",
    "    df_ifdata_cadastro.drop(columns=[c for c in cols_to_drop if c in df_ifdata_cadastro.columns], inplace=True)\n",
    "\n",
    "    # Reordenar colunas\n",
    "    cols_ordem_prioritaria = [\n",
    "        'DATA', 'CNPJ_8', 'NOME_INSTITUICAO_IFD_CAD', 'COD_CONGL_PRUD_IFD_CAD',\n",
    "        'CNPJ_LIDER_8_IFD_CAD', 'COD_CONGL_FIN_IFD_CAD', 'SITUACAO_IFD_CAD',\n",
    "        'DATA_INICIO_ATIVIDADE_IFD_CAD', 'SEGMENTOTB_IFD_CAD', 'ATIVIDADE_IFD_CAD',\n",
    "        'TCB_IFD_CAD', 'TD_IFD_CAD', 'TC_IFD_CAD', 'UF_IFD_CAD', 'MUNICIPIO_IFD_CAD', 'SR_IFD_CAD'\n",
    "    ]\n",
    "    cols_existentes_prioritarias = [c for c in cols_ordem_prioritaria if c in df_ifdata_cadastro.columns]\n",
    "    cols_restantes = [c for c in df_ifdata_cadastro.columns if c not in cols_existentes_prioritarias]\n",
    "    df_ifdata_cadastro = df_ifdata_cadastro[cols_existentes_prioritarias + cols_restantes]\n",
    "\n",
    "    print(\"\\n--- Informações do DataFrame Final: df_ifdata_cadastro ---\")\n",
    "    print(f\"Shape: {df_ifdata_cadastro.shape}\")\n",
    "    print(f\"Colunas: {df_ifdata_cadastro.columns.tolist()}\")\n",
    "    print(f\"Período de dados: {df_ifdata_cadastro['DATA'].min()} a {df_ifdata_cadastro['DATA'].max()}\")\n",
    "    \n",
    "    # --- 4. Salvando o Resultado ---\n",
    "    output_path = dir_outputs / 'df_ifdata_cadastro.parquet'\n",
    "    df_ifdata_cadastro.to_parquet(output_path, index=False)\n",
    "    print(f\"\\nArquivo salvo com sucesso em: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d59d404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CÉLULA 6: Geração de Saídas Auxiliares (Mapeamento, Infos e Dicionários)\n",
    "# ==============================================================================\n",
    "# Descrição: Cria arquivos secundários úteis para a análise, já adaptados\n",
    "# para a nova estrutura de nomes de colunas e com dicionários COSIF separados.\n",
    "# ==============================================================================\n",
    "print(\"\\n>>> GERANDO SAÍDAS AUXILIARES <<<\")\n",
    "\n",
    "# --- 1. Criar DataFrame de Mapeamento (CNPJ -> Conglomerado) ---\n",
    "try:\n",
    "    if not df_cosif_prudencial.empty:\n",
    "        print(\"\\nCriando mapeamento CNPJ -> Conglomerado...\")\n",
    "        df_mapeamento = (\n",
    "            df_cosif_prudencial[['CNPJ_8', 'COD_CONGL_PRUD_COSIF', 'NOME_CONGL_PRUD_COSIF', 'DATA']]\n",
    "            .sort_values('DATA', ascending=False)\n",
    "            .drop_duplicates(subset=['CNPJ_8'], keep='first')\n",
    "            .drop(columns='DATA')\n",
    "            .reset_index(drop=True)\n",
    "            .rename(columns={\n",
    "                'COD_CONGL_PRUD_COSIF': 'COD_CONGL_PRUD',\n",
    "                'NOME_CONGL_PRUD_COSIF': 'NOME_CONGL_PRUD'\n",
    "            })\n",
    "        )\n",
    "        \n",
    "        output_path_map = dir_outputs / 'df_mapeamento_cnpj_conglomerado.parquet'\n",
    "        df_mapeamento.to_parquet(output_path_map, index=False)\n",
    "        print(f\"Shape do mapeamento: {df_mapeamento.shape}\")\n",
    "        print(f\" -> Sucesso! Arquivo de mapeamento salvo em: {output_path_map}\")\n",
    "    else:\n",
    "        print(\"\\nAVISO: df_cosif_prudencial está vazio. O arquivo de mapeamento não será gerado.\")\n",
    "except NameError:\n",
    "    print(\"\\nAVISO: Variável 'df_cosif_prudencial' não foi criada. O arquivo de mapeamento não será gerado.\")\n",
    "\n",
    "\n",
    "# --- 2. Gerar Arquivos de Informações dos DataFrames (Info) ---\n",
    "print(\"\\n--- Gerando Arquivos de Informações (Perfil) dos DataFrames ---\")\n",
    "dfs_to_document = {\n",
    "    'df_cosif_individual': 'cosif_individual',\n",
    "    'df_cosif_prudencial': 'cosif_prudencial',\n",
    "    'df_ifdata_valores': 'ifdata_valores',\n",
    "    'df_ifdata_cadastro': 'ifdata_cadastro'\n",
    "}\n",
    "\n",
    "for df_var_name, filename_prefix in dfs_to_document.items():\n",
    "    if df_var_name in locals() and not locals()[df_var_name].empty:\n",
    "        df_to_process = locals()[df_var_name]\n",
    "        info_df = create_data_infos(df_to_process, filename_prefix)\n",
    "        \n",
    "        output_excel_path = dir_outputs / f'info_dataframe_{filename_prefix}.xlsx'\n",
    "        info_df.to_excel(output_excel_path, index=False, engine='openpyxl')\n",
    "        print(f\" -> Sucesso! Arquivo de info salvo em: {output_excel_path}\\n\")\n",
    "    else:\n",
    "        print(f\"\\nAVISO: DataFrame '{df_var_name}' está vazio ou não existe. Arquivo de info não será gerado.\")\n",
    "\n",
    "\n",
    "# --- 3. Gerar Dicionários de Contas (Código -> Nome) ---\n",
    "print(\"\\n--- Gerando Dicionários de Contas ---\")\n",
    "\n",
    "# MODIFICAÇÃO: Dicionários COSIF agora são gerados separadamente.\n",
    "\n",
    "# Dicionário para COSIF Individual\n",
    "print(\"\\nProcessando dicionário para: COSIF Individual\")\n",
    "try:\n",
    "    if not df_cosif_individual.empty:\n",
    "        dict_cosif_ind = (\n",
    "            df_cosif_individual[['CONTA_COSIF', 'NOME_CONTA_COSIF']]\n",
    "            .drop_duplicates().sort_values('CONTA_COSIF').reset_index(drop=True)\n",
    "        )\n",
    "        path = dir_outputs / 'dicionario_contas_cosif_individual.xlsx'\n",
    "        dict_cosif_ind.to_excel(path, index=False)\n",
    "        print(f\" -> Sucesso! Dicionário salvo em: {path}\")\n",
    "    else:\n",
    "        print(\" -> AVISO: df_cosif_individual vazio ou colunas de conta não encontradas.\")\n",
    "except NameError:\n",
    "    print(\" -> AVISO: DataFrame 'df_cosif_individual' não existe. Dicionário não gerado.\")\n",
    "\n",
    "# Dicionário para COSIF Prudencial\n",
    "print(\"\\nProcessando dicionário para: COSIF Prudencial\")\n",
    "try:\n",
    "    if not df_cosif_prudencial.empty:\n",
    "        dict_cosif_prud = (\n",
    "            df_cosif_prudencial[['CONTA_COSIF', 'NOME_CONTA_COSIF']]\n",
    "            .drop_duplicates().sort_values('CONTA_COSIF').reset_index(drop=True)\n",
    "        )\n",
    "        path = dir_outputs / 'dicionario_contas_cosif_prudencial.xlsx'\n",
    "        dict_cosif_prud.to_excel(path, index=False)\n",
    "        print(f\" -> Sucesso! Dicionário salvo em: {path}\")\n",
    "    else:\n",
    "        print(\" -> AVISO: df_cosif_prudencial vazio ou colunas de conta não encontradas.\")\n",
    "except NameError:\n",
    "    print(\" -> AVISO: DataFrame 'df_cosif_prudencial' não existe. Dicionário não gerado.\")\n",
    "\n",
    "# Dicionário para IFDATA Valores\n",
    "print(\"\\nProcessando dicionário para: IFDATA Valores\")\n",
    "try:\n",
    "    if not df_ifdata_valores.empty:\n",
    "        dict_ifdata = (\n",
    "            df_ifdata_valores[['CONTA_IFD_VAL', 'NOME_CONTA_IFD_VAL']]\n",
    "            .drop_duplicates().sort_values('CONTA_IFD_VAL').reset_index(drop=True)\n",
    "        )\n",
    "        path = dir_outputs / 'dicionario_contas_ifdata_valores.xlsx'\n",
    "        dict_ifdata.to_excel(path, index=False)\n",
    "        print(f\" -> Sucesso! Dicionário salvo em: {path}\")\n",
    "    else:\n",
    "        print(\" -> AVISO: df_ifdata_valores vazio ou colunas de conta não encontradas.\")\n",
    "except NameError:\n",
    "    print(\" -> AVISO: DataFrame 'df_ifdata_valores' não existe. Dicionário não gerado.\")\n",
    "\n",
    "# --- 4. Gerar Dicionário de Entidades (Nomes, CNPJs e Códigos) ---\n",
    "print(\"\\n--- Gerando Dicionário de Entidades para Consulta ---\")\n",
    "\n",
    "try:\n",
    "    # Passo 1: Criar a base de dados de entidades a partir do IFDATA Cadastro, incluindo todos os identificadores\n",
    "    # Selecionamos a entrada mais recente para cada CNPJ_8 para obter os códigos mais atuais.\n",
    "    df_base_entidades = (\n",
    "        df_ifdata_cadastro[[\n",
    "            'DATA', 'CNPJ_8', 'NOME_INSTITUICAO_IFD_CAD', \n",
    "            'COD_CONGL_PRUD_IFD_CAD', 'COD_CONGL_FIN_IFD_CAD', 'CNPJ_LIDER_8_IFD_CAD'\n",
    "        ]]\n",
    "        .sort_values('DATA', ascending=False)\n",
    "        .drop_duplicates(subset=['CNPJ_8'], keep='first')\n",
    "        .drop(columns='DATA')\n",
    "        .rename(columns={\n",
    "            'NOME_INSTITUICAO_IFD_CAD': 'NOME_PRINCIPAL',\n",
    "            'COD_CONGL_PRUD_IFD_CAD': 'COD_CONGL_PRUD',\n",
    "            'COD_CONGL_FIN_IFD_CAD': 'COD_CONGL_FIN',\n",
    "            'CNPJ_LIDER_8_IFD_CAD': 'CNPJ_LIDER'\n",
    "        })\n",
    "    ).dropna(subset=['CNPJ_8'])\n",
    "\n",
    "    # Passo 2: Coletar todos os nomes únicos e seus CNPJs de todas as fontes\n",
    "    nomes_cosif_prud = df_cosif_prudencial[['CNPJ_8', 'NOME_INSTITUICAO_COSIF']].rename(columns={'NOME_INSTITUICAO_COSIF': 'NOME_VARIACOES'})\n",
    "    nomes_cosif_ind = df_cosif_individual[['CNPJ_8', 'NOME_INSTITUICAO_COSIF']].rename(columns={'NOME_INSTITUICAO_COSIF': 'NOME_VARIACOES'})\n",
    "    nomes_ifd_cad = df_ifdata_cadastro[['CNPJ_8', 'NOME_INSTITUICAO_IFD_CAD']].rename(columns={'NOME_INSTITUICAO_IFD_CAD': 'NOME_VARIACOES'})\n",
    "    \n",
    "    df_todos_nomes = (\n",
    "        pd.concat([nomes_cosif_prud, nomes_cosif_ind, nomes_ifd_cad])\n",
    "        .dropna()\n",
    "        .drop_duplicates()\n",
    "        .groupby('CNPJ_8')['NOME_VARIACOES']\n",
    "        .apply(lambda x: ' | '.join(sorted(list(set(x)))))\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Passo 3: Fazer o merge da base de entidades com a lista de todos os nomes\n",
    "    df_entidades_final = pd.merge(\n",
    "        df_base_entidades,\n",
    "        df_todos_nomes,\n",
    "        on='CNPJ_8',\n",
    "        how='outer'\n",
    "    )\n",
    "    \n",
    "    # Preencher o nome principal caso ele esteja faltando (para entidades que só existem no COSIF)\n",
    "    df_entidades_final['NOME_PRINCIPAL'] = df_entidades_final['NOME_PRINCIPAL'].fillna(\n",
    "        df_entidades_final['NOME_VARIACOES'].str.split(' | ').str[0]\n",
    "    )\n",
    "    \n",
    "    # Reordenar as colunas para uma apresentação lógica\n",
    "    cols_ordem = [\n",
    "        'NOME_PRINCIPAL', \n",
    "        'CNPJ_8', \n",
    "        'CNPJ_LIDER', \n",
    "        'COD_CONGL_PRUD', \n",
    "        'COD_CONGL_FIN', \n",
    "        'NOME_VARIACOES'\n",
    "    ]\n",
    "    df_entidades_final = df_entidades_final[cols_ordem]\n",
    "    df_entidades_final = df_entidades_final.sort_values('NOME_PRINCIPAL').reset_index(drop=True)\n",
    "\n",
    "    # Salvar em Excel\n",
    "    output_path_entidades = dir_outputs / 'dicionario_entidades.xlsx'\n",
    "    df_entidades_final.to_excel(output_path_entidades, index=False)\n",
    "    \n",
    "    print(f\"-> {len(df_entidades_final)} entidades únicas salvas com sucesso em: {output_path_entidades}\")\n",
    "    print(\"O dicionário agora contém todos os identificadores chave (Líder, Prudencial, Financeiro).\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERRO ao gerar o dicionário de entidades: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
